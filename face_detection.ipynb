{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ccd79d",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0df8c2-5e7e-4c8d-b4ad-1bcf11b4d24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\KHUSHI\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f641fc23",
   "metadata": {},
   "source": [
    "### Initialize Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d102d87-3e12-4021-947c-d7282b236c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Face Detection and Drawing\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_draw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f493ab",
   "metadata": {},
   "source": [
    "### Initialize video capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd0686f0-24bc-4f55-9b31-dba01c26df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aac4be",
   "metadata": {},
   "source": [
    "### Intializes face detection and emotion analysis ,and stops the video capture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8450efa-9d7a-4215-9cba-3bb95740a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_face_detection.FaceDetection(min_detection_confidence=0.5) as face_detection:\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty frame...\")\n",
    "            continue\n",
    "\n",
    "        # Convert the frame to RGB as MediaPipe processes RGB images\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Perform face detection\n",
    "        results = face_detection.process(rgb_frame)\n",
    "\n",
    "        # Draw detection results on the frame and perform emotion detection\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                # Draw bounding box around the face\n",
    "                mp_draw.draw_detection(frame, detection)\n",
    "\n",
    "                # Get bounding box coordinates\n",
    "                bbox_c = detection.location_data.relative_bounding_box\n",
    "                h, w, _ = frame.shape\n",
    "                bbox = int(bbox_c.xmin * w), int(bbox_c.ymin * h), \\\n",
    "                       int(bbox_c.width * w), int(bbox_c.height * h)\n",
    "\n",
    "                # Extract the face from the frame\n",
    "                face = frame[bbox[1]:bbox[1] + bbox[3], bbox[0]:bbox[0] + bbox[2]]\n",
    "\n",
    "                # Perform emotion detection\n",
    "                try:\n",
    "                    analysis = DeepFace.analyze(face, actions=['emotion'], enforce_detection=False)\n",
    "                    \n",
    "                    # Handle analysis output structure\n",
    "                    if isinstance(analysis, dict):\n",
    "                        emotion = analysis.get('dominant_emotion', 'Unknown')\n",
    "                    elif isinstance(analysis, list) and len(analysis) > 0:\n",
    "                        emotion = analysis[0].get('dominant_emotion', 'Unknown')\n",
    "                    else:\n",
    "                        emotion = 'Unknown'\n",
    "                    \n",
    "                    # Display detected emotion on the frame\n",
    "                    cv2.putText(frame, f'Emotion: {emotion}', (bbox[0], bbox[1] - 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in emotion detection: {e}\")\n",
    "\n",
    "                # Displaying the confidence score on the frame\n",
    "                score = detection.score[0]\n",
    "                cv2.putText(frame, f'Score: {int(score * 100)}%', (bbox[0], bbox[1] - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the output frame\n",
    "        cv2.imshow('Face Detection with Emotion', frame)\n",
    "\n",
    "        # Break the loop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture object and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0927be0c-54d4-4dae-a2c3-9e345dac9451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
